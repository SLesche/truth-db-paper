# Method
## The Truth Effect Database (TED)

Resources to build the database, restructure raw data, build the website and integrate the submitted data can be found on Github (https://github.com/SLesche/truth-effect-database). We also provide a landing page with detailed information on the project and a user-guide for interacting with the database (https://slesche.github.io/ted-site/).

The database is implemented using Structured Query Language (SQL), specifically SQLite (Hipp, 2020). A key advantage of SQLite is its serverless architecture – data is stored in a single, shareable file, allowing researchers to download, interact with, and modify the database locally without the need for a dedicated database server.

As a relational database system, SQLite supports the use of multiple interconnected tables. This structure enables efficient organization of complex data relationships. For instance, each publication may include several studies, which themselves involve multiple experimental conditions. By organizing data across discrete, normalized tables rather than in a single flat file, TED minimizes redundancy and enhances both storage efficiency and query performance.

The design of our database is illustrated in Figure 1. Broadly, we make use of a table for each part of data related to truth effect experiments. Meta-data concerning the publications themselves as the highest order are stored in a publication_table and raw data at the lowest level in the observation_table. Between these two levels, additional tables capture detailed information about studies, procedures, materials, and collected measures:
•	*publication_table*: Contains meta-data about the publication (e.g., authors, year, journal).
•	*study_table*: Includes study-level information such as participant characteristics, use of filler tasks, and the location of raw data in online repositories.
•	*procedure_table*: Describes the experimental procedure, including commonly manipulated variables in truth effect research (e.g., type of repetition, delay between exposure and judgment phase, warnings about the truth effect).
•	*between_table*: Lists between-subject manipulations not already encoded in the *procedure_table.*
•	*within_table*: Lists within-subject manipulations not already encoded in the *procedure_table.*
•	*statementset_table*: Provides information about the publication or source from which a set of statements was retrieved.
•	*statement_table*: Contains the text of each statement and its accuracy classification.
•	*measure_table*: Includes information about additional measures collected during the experiment. Although raw data for these measures are not included due to limited overlap across studies, researchers can use this table to filter relevant studies and retrieve corresponding raw data separately.
•	*observation_table*: Stores the harmonized raw trial-level data, representing the lowest level of the database structure.


(ref:database-overview-plot) Overview of TED Structure
```{r database-overview-plot,  fig.cap = paste("(ref:database-overview-plot)"), out.width="100%"}
knitr::include_graphics(
 path = "./images/ted-overview.png"
)
```

This relational approach provides a clear and modular organization of data sources, linking tables through unique identifier variables. We argue that while the use of a relational database adds some complexity, it also introduces an intuitive naming system and structure for variables of interest. Importantly, our goal is that variable names and their table location are the only knowledge that users need to have in order to interact with the database. To this end, we have developed tools that require little to no understanding of the database structure or SQL in order to submit and extract data from the database.

### Data Submission
To ease the process of submitting data to the database, we built a website (https://slesche.github.io/truth-effect-database/) that guides users through the submission process and checks submitted information for inconsistencies or errors (see Figure \@ref(fig:submission-overview-png)). This website is designed to minimize both the effort of submitting new data to the database as well as proactively identify potential errors in the submission process. 

(ref:submission-overview-png) Screenshot of the TED data entry webpage
```{r submission-overview-png,  fig.cap = paste("(ref:submission-overview-png)"), out.width="100%"}
knitr::include_graphics(
 path = "./images/screenshot_submission_homepage.png"
)
```

Contributors are prompted to provide information across all levels of the database, including publication metadata, study characteristics, experimental conditions, statements, and raw trial data. Importantly, this also requires formatting the raw data according to the documented TED variable names. By placing the responsibility for mapping and labeling variables on the researchers most familiar with the dataset, ambiguities about variable meaning are resolved at the source and the data are directly translated into a standardized format that is transparent and usable for the broader community. 

The system cross-checks entered metadata against the uploaded raw data (e.g., verifying the number and coding of conditions or statements) and provides immediate feedback if inconsistencies arise. These automated validation steps ensure that submissions are internally coherent and can be smoothly integrated into the harmonized database structure, while maintaining a low barrier to contribution.

Once all required information has been entered and validated, contributors finalize their submission by clicking the “Submit Data” button. This action generates and downloads a finalized JSON file containing the complete submission package. Contributors then send this file to the TED maintainers for review and final approval, after which the data are integrated into the database.

Data can be submitted at any stage of the research process (e.g. immediately after data collection, upon release of a preprint, or following formal publication). If the status of the associated manuscript changes, contributors can easily update this information by contacting the TED maintainers and informing them of the new status.

### Data Extraction
We extended the R package *acdcquery* [@R-acdcquery] to simplify the process of extracting data. This package provides functions to facilitate connection to the database and extraction of data. Users can define filter arguments using `add_argument()` and request specific variables from any table in the database using `query_db()`. 

For example, users may request all available trial-level data from studies with greater than 200 participants from the test phase using the code below:

```{r, eval = FALSE, echo = TRUE}
library(acdcquery)

download_ted("path/to/")

# OR check whether your version is up-to-date using check_ted("path/to/ted.db")
# and update it using update_ted("path/to/ted.db")

conn <- connect_to_db("path/to/ted.db")

arguments <- list() %>% add_argument(
    conn = conn,
    variable = "publication_id", # You can use any variable name from TED, no need to specify its table
    operator = "greater",
    values = 0
  ) %>%
  add_argument(  # You can chain together multiple arguments
    conn = conn,
    variable = "phase", # You can use any variable name from TED, no need to specify its table
    operator = "equal",
    values = "test"
  )

procedure_results <- query_db(
  conn = conn,
  arguments = arguments,
  target_vars = c("default", "publication_id"), # "default" will automatically return all vars in "target_table"
  target_table = "procedure_table"
)

trial_results <- query_db(
  conn = conn,
  arguments = arguments,
  target_vars = c("default", "phase"), # You can add any variable from any table to be added to the result
  target_table = "observation_table"
)
```

The R package will automatically construct the necessary SQL query and return the filtered data with the selected columns to the user. A detailed user guide on using the R package to interact with our database can be found on the databases GitHub (https://github.com/SLesche/truth-effect-database).

### Data Selection
We included only data from studies specifically focused on the repetition-based illusory truth effect. To qualify for inclusion, studies had to involve participants making truth judgments about visual or auditory stimuli, with a subset of those stimuli having been presented previously to allow for repetition effects. Our data selection process was based in part on the work of Henderson et al. (2022), who identified 17 publications with accessible raw data. Additionally, we contacted colleagues directly to request openly available datasets, further expanding the scope of included studies and conducted a literature search to find open data from the past years.

### Data Analysis 
To illustrate the analytical potential of TED, we conducted a bayesian multilevel model predicting truth judgments, incorporating both fixed and random effects of repetition at the statement, subject, and procedure (i.e., experimental) levels. The scale and structure of TED enable us to estimate the variance in the repetition effect simultaneously across all three levels. This approach allows us to determine whether the size of the truth effect varies more substantially across individuals, across experimental procedures, or across the specific statements used.