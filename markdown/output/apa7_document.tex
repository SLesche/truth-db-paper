% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man,floatsintext]{apa7}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\makeatother
\keywords{keyword, more keyword, crazy keyword\newline\indent Word count: X}
\usepackage{csquotes}
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\raggedbottom

\usepackage{hhline}

\setlength{\parskip}{0pt}

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Introducing the Truth Effect Database (TED) - an open trial-level database promoting FAIR data in truth effect research},
  pdfauthor={Sven Lesche1 \& Annika Stump2},
  pdflang={en-EN},
  pdfkeywords={keyword, more keyword, crazy keyword},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Introducing the Truth Effect Database (TED) - an open trial-level database promoting FAIR data in truth effect research}
\author{Sven Lesche\textsuperscript{1} \& Annika Stump\textsuperscript{2}}
\date{}


\shorttitle{Introducing TED}

\authornote{

Author Notes go here.

The authors made the following contributions. Sven Lesche: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Annika Stump: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing.

Correspondence concerning this article should be addressed to Sven Lesche, Hauptstraße 47-51, 69117 Heidelberg. E-mail: \href{mailto:sven.lesche@psychologie.uni-heidelberg.de}{\nolinkurl{sven.lesche@psychologie.uni-heidelberg.de}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Ruprecht-Karls-University Heidelberg\\\textsuperscript{2} Albert-Ludwigs-University Freiburg}

\abstract{%
One or two sentences providing a \textbf{basic introduction} to the field, comprehensible to a scientist in any discipline.

Two to three sentences of \textbf{more detailed background}, comprehensible to scientists in related disciplines.

One sentence clearly stating the \textbf{general problem} being addressed by this particular study.

One sentence summarizing the main result (with the words ``\textbf{here we show}'' or their equivalent).

Two or three sentences explaining what the \textbf{main result} reveals in direct comparison to what was thought to be the case previously, or how the main result adds to previous knowledge.

One or two sentences to put the results into a more \textbf{general context}.

Two or three sentences to provide a \textbf{broader perspective}, readily comprehensible to a scientist in any discipline.
}



\begin{document}
\maketitle

\begin{verbatim}
## [1] "Querying through observation table. Query times may be longer."
\end{verbatim}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The open science movement has emerged in response to growing concerns about the replicability, transparency, and efficiency of scientific research (\protect\hyperlink{ref-ioannidis2005most}{Ioannidis, 2005}; \protect\hyperlink{ref-munafo2017manifesto}{Munafò et al., 2017}; \protect\hyperlink{ref-nosek2015promoting}{Nosek et al., 2015}). At its core, open science promotes practices that make research processes and outputs accessible, verifiable, and reusable. Practices such as preregistration, data sharing, and open materials aim to increase the trustworthiness of findings and to foster a more cumulative, collaborative scientific enterprise. These practices, coupled with platforms like the Open Science Framework - OSF (\protect\hyperlink{ref-fosterOpenScienceFramework2017}{Foster \& Deardorff, 2017}), have played a pivotal role in advancing openness through tools that support study registration, version control, and public sharing of data and materials. Transparency, once rare, is increasingly becoming the norm (\protect\hyperlink{ref-bauer2022psychological}{Bauer, 2022}).

However, while transparency has improved markedly, efficiency and reusability have lagged behind. Research materials and datasets are often siloed within individual project pages, tailored to host single studies. As a result, even when data are technically open, they are rarely standardized in ways that facilitate reuse, integration, or cumulative analysis. Current open data practices focus primarily on the first two aspects of the FAIR (Findable, Accessible, Interoperable, and Reusable) principles of data sharing (\protect\hyperlink{ref-wilkinson2016fair}{Wilkinson et al., 2016}), while neglecting the latter. For example, the OSF is well-suited to documenting and finding individual projects, but its structure does not support the aggregation or harmonization of data across multiple related studies. In addition to structural limitations, researchers' practices like inconsistent file formats, project-specific codebooks, and divergent analytic pipelines continue to hinder interoperability.

This inconsistency not only limits replicability, but also reusability. Should a researcher be interested in estimating an effect size, developing a new computational model or simply re-analyzing existing data instead of collecting an entirely new sample, they have to comb through OSF, pray for existing codebooks and hope for decipherable raw data. This is both tedious and often unsuccessful (\protect\hyperlink{ref-cruwell2023s}{Crüwell et al., 2023}; \protect\hyperlink{ref-hardwicke2021analytic}{Hardwicke et al., 2021}). However, reusability greatly improves scientific efficiency. It saves resources otherwise wasted by planning and collecting an entirely new set of data and helps efficiently allocate public resources as well as participants' and researchers' time.

We argue that to make research outputs genuinely reusable, not just transparent, structure is essential. Reusability requires standardized organization: consistent variable naming, common data schemas, uniform documentation practices, and clear, accessible codebooks. In neuroscience, this principle has been successfully realized through the Brain Imaging Data Structure - BIDS (\protect\hyperlink{ref-gorgolewski2016brain}{Gorgolewski et al., 2016}), which provides a framework for organizing and annotating neuroimaging data. BIDS has enabled not just more efficient sharing, but also reproducible pipelines, automated analyses, and collaborative efforts at scale (\protect\hyperlink{ref-poldrack2024past}{Poldrack et al., 2024}).

Structured approaches like BIDS are now gaining traction in cognitive psychology. The Attentional Control Data Collection (ACDC) introduced a trial-level database for data from attentional control experiments, providing standardized variable names, metadata, and analysis-ready formats that streamline cross-study comparisons (\protect\hyperlink{ref-haaf2025attentional}{Haaf et al., 2025}). Similarly, the FEARBASE project is building a large-scale, open-access repository for fear conditioning studies, adopting a shared structure to ensure comparability and long-term usability (Lohnsdorf \& Ehlers, 2025). Crucially, there is increasing institutional support for the development of infrastructure enabling data reuse. For example, the German research foundation \emph{DFG} issued an open funding call for projects developing infrastructure for scientific data management (DFG; \emph{Förderprogramm Informationsinfrastrukturen für Forschungsdaten}). These projects spearhead a changing culture of data-reusability but so far represent the exception, rather than the norm.

We believe that there is great potential in the development of structured databases integrating trial level data. Large and structured collections of raw data have several key applications, extending the general principle of data-reusability. They enable living meta-analyses that can update automatically as new data are added; allow researchers to find relevant raw data based on task characteristics or participant variables; facilitate the straightforward replication of published findings; and enable power analyses using pooled datasets from comparable studies.

Beyond these direct applications, structured datasets also support the development of new methods and models. Trial-level repositories allow for benchmarking existing analytic tools, training machine learning models, and exploring novel statistical approaches.

To illustrate the potential of such structured and reusable databases, we focus on a particularly relevant and empirically rich phenomenon in cognitive and social psychology: the illusory truth effect (ITE). The ITE has been robustly demonstrated across numerous studies, yet substantial heterogeneity in effect sizes, experimental designs, and individual-level moderators complicates efforts to draw generalizable conclusions. These complexities make it an ideal case for a centralized, harmonized repository of trial-level data. In the following section, we provide a brief overview of the ITE and explain how its empirical challenges underscore the need for exactly the kind of infrastructure we propose.

\hypertarget{the-illusory-truth-effect}{%
\subsubsection{The illusory truth effect}\label{the-illusory-truth-effect}}

The ITE exemplifies a psychological mechanism with both theoretical significance and pressing real-world relevance. In today's data-saturated and fast-paced information environment, individuals are routinely exposed to repeated content of uncertain credibility --- from benign repetition to coordinated misinformation. As such, understanding how repetition influences perceived truth is central to addressing challenges in belief formation, misinformation spread, and public trust. Originally demonstrated by Hasher et al.~(1977), the ITE refers to the tendency to perceive repeated information as more credible than novel ones, regardless of their actual accuracy. Since then, the effect has been robustly replicated in over 80 studies.

Previous research indicates that the illusory truth effect is primarily driven by increased processing fluency as a result of repetition. That is, when evaluating the truthfulness of a statement, individuals often draw on their subjective sense of ease during processing as a heuristic cue (e.g., Dechêne et al., 2010). Crucially, the relative fluency appears to be decisive: Dechêne et al.~(2009) showed that the effect only reliably occurs when fluent and disfluent stimuli are presented together. In experimental paradigms investigating the ITE, fluency is typically manipulated by contrasting new statements with repeated ones which have been shown during a prior exposure phase.
Although ITE has been observed across various domains and populations, prior research indicates that the illusory truth effect significantly varies among individuals (e.g., Nadarevic, 2010; Schnuerch et al., 2021).

Research examining dispositional factors that may account for individual differences in susceptibility to the illusory truth effect has yielded mixed and often inconclusive findings. For example, Arkes et al.~(1991) and Boehm (1994) investigated need for cognition (NFC) as a potential trait moderator of the effect but found no supporting evidence. Similarly, Newman et al.~(2020) explored whether NFC moderates the illusory truth effect and found that this relationship was contingent upon the nature of the experimental instructions. Specifically, the moderating influence of NFC emerged only when participants were not informed that the exposure phase contained both true and false statements; when such information was provided, the effect was no longer evident. In addition to NFC, other dispositional factors have been investigated with similarly inconsistent results. For instance, Kim (2002) examined whether skepticism moderates the effect and reported ambiguous findings, while DiFonzo et al.~(2016) found marginal evidence for such moderation. Sundar et al.~(2015), focusing on repeated health-related messages, observed that individuals with a high need for affect (i.e., the tendency to approach or avoid affect-inducing situations) were more susceptible to the ITE. More recently, De keersmaecker et al.~(2020) tested three further individual difference variables -- need for cognitive closure, thinking style preference (intuitive vs.~deliberative), and cognitive ability -- but found no evidence that any of these measures moderated the truth effect. In two experiments, Stump et al.~(2022) found evidence suggesting moderating effects for NCC on the illusory truth effect after a relatively short but not after a longer retention interval (10 minutes vs.~1 week). Crucially, De keersmaecker et al.~(2020) assessed NCC exclusively in one study, which employed a single, relatively long retention interval of five to seven days. The pattern of findings suggests that the temporal distance between exposure and judgment phase may be a key factor when assessing the role of individual differences in the truth effect. Further support for this interpretation comes from recent work by Schnuerch et al.~(2021), who conducted a comprehensive Bayesian reanalysis of multiple datasets examining individual differences in the illusory truth effect. Their results revealed robust evidence for such differences in five datasets, whereas at least two datasets provided evidence against them. Crucially, in the latter two studies, the retention interval spanned one week, while in the other datasets the judgment phase occurred within the same experimental session as the exposure phase.

Taken together, these findings offer preliminary insights into how individual differences may shape susceptibility to the truth effect. However, they also highlight the complexity of the phenomenon and suggest that an integrative approach -- considering both dispositional and contextual variables -- is essential for a more comprehensive understanding of the mechanisms underlying the ITE.

\hypertarget{the-present-study}{%
\subsubsection{The present study}\label{the-present-study}}

In the spirit of providing FAIR data to aid research on ITE, we introduce a centralized, trial-level database, drawing on resources and lessons learned from developing ACDC (\protect\hyperlink{ref-haaf2025attentional}{Haaf et al., 2025}). The truth effect database (TED) is designed not only with structured organization in mind, but also with an emphasis on ease of use. Particular attention has been given to lowering the barrier for data submission through an intuitive entry-mask, as well as to enabling simple data extraction via an R package. Our aim is to create a living, extensible resource that supports both contributors and users: researchers can add new studies with minimal friction, and others can search, filter, and analyze trial-level data without having to clean or realign disparate datasets. By combining structure with usability, this resource is intended to foster cumulative research on the illusory truth effect and to serve as a model for reusable psychological data infrastructures more broadly.

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{the-truth-effect-database-ted}{%
\subsection{The Truth Effect Database (TED)}\label{the-truth-effect-database-ted}}

Resources to build the database, restructure raw data, build the website and integrate the submitted data can be found on Github (\url{https://github.com/SLesche/truth-effect-database}).

The database is implemented using Structured Query Language (SQL), specifically \emph{SQLite} (\protect\hyperlink{ref-sqlite2020hipp}{Hipp, 2020}). A key advantage of SQLite is its serverless architecture---data is stored in a single, shareable file, allowing researchers to download, interact with, and modify the database locally without the need for a dedicated database server.

As a relational database system, SQLite supports the use of multiple interconnected tables. This structure enables efficient organization of complex data relationships. For instance, each publication may include several studies, which themselves involve multiple experimental conditions. By organizing data across discrete, normalized tables rather than in a single flat file, TED minimizes redundancy and enhances both storage efficiency and query performance.

The design of our database is illustrated in Figure \ref{fig:database-overview-plot}. Broadly, we make use of a table for each part of data related to truth effect experiments. Meta-data concerning the publications themselves as the highest order are stored in a \emph{publication\_table} and raw data at the lowest level in the \emph{observation\_table}. In addition, we included information on the study, the conditions, the procedure, the statements used, their origin, and additional variables collected in the experiment in respective tables.



\begin{figure}
\includegraphics[width=1\linewidth]{./images/ted-overview} \caption{Overview of TED Structure}\label{fig:database-overview-plot}
\end{figure}

We argue that while the use of a relational database adds some complexity, it also introduces an intuitive naming system and structure for variables of interest. Importantly, our goal is that variable names and their table location are the only knownledge that users need to have in order to interact with the database. To this end, we have developed tools that require little to no understanding of the database structure or SQL in order to submit and extract data from the database.

\hypertarget{data-submission}{%
\subsubsection{Data Submission}\label{data-submission}}

To ease the process of submitting data to the database, we built a website (\url{https://slesche.github.io/truth-effect-database/}) that guides users through the submission process and checks submitted information for inconsistencies or errors. This website is designed to minimize both the effort of submitting new data to the database as well as identify potential errors in the submission process.

\hypertarget{data-extraction}{%
\subsubsection{Data Extraction}\label{data-extraction}}

We extended the R package \emph{acdcquery} (\protect\hyperlink{ref-R-acdcquery}{Lesche, 2025}) to simplify the process of extracting data. This package provides functions to facilitate connection to the database and extraction of data. Users can define filter arguments using \texttt{add\_argument()} and request specific variables from any table in the database using \texttt{query\_db()}.

For example, users may request all available trial-level data from studies with greater than 200 participants from the test phase using the code below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(acdcquery)}

\NormalTok{conn }\OtherTok{\textless{}{-}} \FunctionTok{connect\_to\_db}\NormalTok{(}\StringTok{"path/to/ted.db"}\NormalTok{)}

\NormalTok{arguments }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_argument}\NormalTok{(}
\NormalTok{    conn,}
    \StringTok{"n\_participants"}\NormalTok{,}
    \StringTok{"greater"}\NormalTok{,}
    \DecValTok{200}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_argument}\NormalTok{(}
\NormalTok{    conn,}
    \StringTok{"phase"}\NormalTok{,}
    \StringTok{"equal"}\NormalTok{,}
    \StringTok{"test"}
\NormalTok{  )}

\NormalTok{results }\OtherTok{\textless{}{-}} \FunctionTok{query\_db}\NormalTok{(}
\NormalTok{  conn, }
\NormalTok{  arguments,}
  \AttributeTok{target\_vars =} \StringTok{"default"}\NormalTok{, }\CommentTok{\# Returns all variables in target\_table}
  \AttributeTok{target\_table =} \StringTok{"observation table"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The R package will automatically construct the necessary SQL query and return the filtered data with the selected columns to the user. A detailed user guide on using the R package to interact with our database can be found on the databases GitHub (\url{https://github.com/SLesche/truth-effect-database}).

\hypertarget{data-selection}{%
\subsubsection{Data Selection}\label{data-selection}}

We included only data from studies specifically focused on the repetition-based illusory truth effect. To qualify for inclusion, studies had to involve participants making truth judgments about visual or auditory stimuli, with a subset of those stimuli having been presented previously to allow for repetition effects. Our data selection process was based in part on the work of Henderson et al. (\protect\hyperlink{ref-hendersonReproducibleSystematicMap2022}{2022}), who identified 17 publications with accessible raw data. Additionally, we contacted colleagues directly to request openly available datasets, further expanding the scope of included studies and conducted a literature search to find open data from the past 3 years.

\hypertarget{data-analysis}{%
\subsubsection{Data Analysis}\label{data-analysis}}

To illustrate the analytical potential of TED, we conducted a bayesian multilevel model predicting truth judgments, incorporating both fixed and random effects of repetition at the statement, subject, and procedure (i.e., study) levels. The scale and structure of TED enable us to estimate the variance in the repetition effect simultaneously across all three levels. This approach allows us to determine whether the size of the truth effect varies more substantially across individuals, across experimental procedures, or across the specific statements used.

\hypertarget{results}{%
\section{Results}\label{results}}

All analysis were conducted using R {[}Version 4.4.1; R Core Team (\protect\hyperlink{ref-R-base}{2022}){]}\footnote{We, furthermore, used the R-packages \emph{acdcquery} (Version 1.0.1; \protect\hyperlink{ref-R-acdcquery}{Lesche, 2025}), \emph{brms} (Version 2.22.0; \protect\hyperlink{ref-R-brms_a}{Bürkner, 2017}, \protect\hyperlink{ref-R-brms_b}{2018}, \protect\hyperlink{ref-R-brms_c}{2021}), \emph{knitr} (Version 1.50; \protect\hyperlink{ref-R-knitr}{Xie, 2015}), \emph{lme4} (Version 1.1.35.4; \protect\hyperlink{ref-R-lme4}{Bates et al., 2015}), \emph{papaja} (Version 0.1.3; \protect\hyperlink{ref-R-papaja}{Aust \& Barth, 2022}) and \emph{tidyverse} (Version 2.0.0; \protect\hyperlink{ref-R-tidyverse}{Wickham et al., 2019}).}.

.

In the current version of the manuscript, we included 56 studies from 27 publications, spanning 12002 participants contributing 778741 trials. A complete list of the included publications can be found in Table \textbf{table with refs, studies, trials here}.

Sample composition ranged from 29 to 949 participants. On average, studies included 218.04 participants (\(\mu_{age} =\) 33.17,\(\sigma_{age} =\) 7.22). An overview of the rating scale usage for truth judgments and the use of a filler task over all included studies can be found in Figure \ref{fig:study-overview-plot}.



\begin{figure}
\includegraphics[width=0.9\linewidth]{images/study_overview_plot} \caption{Overview of Study-related variables in TED}\label{fig:study-overview-plot}
\end{figure}

On average, studies employed 62.90 (\(SD =\) 39.97) statements per
participant in the judgment session and in 88.64 \% of procedure settings exactly 50\% of statements were repeated. Of 88 judgment phases, 75.00 \% were conducted on the same day as the exposure phase. The average delay between exposure and judgment phase if both were conducted on the same day was 3.77 minutes. The average delay between exposure and judgment phase, given the judgment phase was conducted at least one day after the exposure phase, was 7.45 days. An overview of additional variables pertaining to the procedure of the included studies can be found in Figure \ref{fig:procedure-overview-plot}.



\begin{figure}
\includegraphics[width=1\linewidth]{images/procedure_overview_plot} \caption{Overview of Procedure-related variables in TED}\label{fig:procedure-overview-plot}
\end{figure}

Detailed information on the statements presented is available for 53 out of 56 studies. Data on the accuracy of a statement is available for 52.41 \% of trials, the exact statement text is available for 44.72 \% of trials, and response times are available for 16.21 \% of trials.

\hypertarget{multilevel-modelling}{%
\subsection{Multilevel Modelling}\label{multilevel-modelling}}

To illustrate the benefits of our large collection of trial-level data, we applied bayesian multilevel models predicting truth judgments with repetition as a fixed effect and random intercepts and slopes at the subject, statement, and procedure levels.

We analyzed the dichotomous and Likert-type response formats separately due to differences in their scale characteristics. Dichotomous responses (e.g., true/false) require logistic models, whereas Likert-type responses (e.g., 1--5 ratings) allow for linear models. All responses were maximum-normalized to the range 0-1 with one representing the maximum possible response indicating a ``true'' judgment. The repetition status was mean-centered to aid model estimation, a new statement was coded -0.5 and a repeated statement 0.5.

We ran all models using 4 chains with 3000 iterations per chain, 1000 of which were discarded as warmup-samples, leading to a total of 8000 posterior samples. There were no divergent transition, no \(\hat{R} > 1.1\), and visual inspection confirmed that the chains mixed well. We used weakly informative priors for the intercept, fixed effect, and standard deviations for all models.

\[Intercept \sim Normal(0.5, 0.5)\]
\[b \sim Normal(0, 1)\]
\[\sigma \sim Gamma(1, 4)\]

\hypertarget{dichotomous-truth-judgments}{%
\subsubsection{Dichotomous Truth Judgments}\label{dichotomous-truth-judgments}}

The analysis was based on 112399 trials nested within 1576 subjects, 997 statements, and 14 procedures.

Table TODO provides a summary of parameter estimates. As expected, the model indicated a significant fixed effect of repetition (\(OR =\) 1.79, \(95\% \ CrI =\) {[}1.51, 2.12{]}). Notably, the standard deviation of the random slope of repetition was highest at the subject level (\(\sigma =\) 0.72, \(95\% \ CrI =\) {[}0.68, 0.77{]}), followed by the procedure level (\(\sigma =\) 0.28, \(95\% \ CrI =\) {[}0.18, 0.44{]}), and the statement level (\(\sigma =\) 0.13, \(95\% \ CrI =\) {[}0.03, 0.19{]}).

\hypertarget{scale-truth-judgments}{%
\subsubsection{Scale Truth Judgments}\label{scale-truth-judgments}}

The analysis was based on 572775 trials nested within 8309 subjects, 2872 statements, and 65 procedures.

Table TODO provides a summary of parameter estimates. As expected, the model indicated a significant fixed effect of repetition (\(b =\) 0.08, \(95\% \ CrI =\) {[}0.07, 0.10{]}). Again, the standard deviation of the random slope of repetition was highest at the subject level (\(\sigma =\) 0.10, \(95\% \ CrI =\) {[}0.10, 0.10{]}), followed by the procedure level (\(\sigma =\) 0.07, \(95\% \ CrI =\) {[}0.05, 0.08{]}), and the statement level (\(\sigma =\) 0.03, \(95\% \ CrI =\) {[}0.02, 0.03{]}).

To further explore the influence of temporal delay between the exposure and judgment phases on inter-individual variability in the repetition effect, we included an interaction between subject and temporal delay (same-day vs.~different day) in the random effect structure. This effectively introduces two sources of variance in the intercept and repetition effect with separate standard deviations. We can then investigate whether the difference in the standard deviation of the random effect of repetition on the subject-level is different depending on the temporal delay.

Table TODO provides a summary of parameter estimates. The standard deviation of the random slope of repetition at the subject level for a same-day judgment phase was \(\sigma_0 =\) 0.11 (\(95\% \ CrI =\) {[}0.10, 0.11{]}). The standard deviation for the random slope on a later day judgment phase was \(\sigma_1 =\) 0.08 (\(95\% \ CrI =\) {[}0.08, 0.09{]}). The difference in standard deviations in the random effect of repetition at the subject level deviated substantially from zero \(\sigma_0 - \sigma_1 =\) 0.02 (\(95\% \ CrI =\) {[}0.02, 0.02{]}).

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In this paper, we introduce the Truth Effect Database (TED), a large-scale, open-access repository of trial-level data on truth judgments. TED is designed to support cumulative, reproducible research on the truth effect by standardizing and centralizing data from a wide range of studies. It currently aggregates data from 27 publications, spanning 12002 participants and 778741 trials, accounting for a wide range of dispositional and contextual variables -- including experimental instructions, task during initial exposure, number of repetitions, and length of the retention interval. With its growing size, TED provides a critical resource for investigating how repetition shapes belief formation -- a process of increasing relevance in the digital age, where repeated exposure to false or misleading information is pervasive.

Our goal in developing TED was to address key challenges in open research data: the fragmentation of data across studies, the lack of standard formats for sharing experimental data, and the resulting lack of interoperability and reusability (\protect\hyperlink{ref-cruwell2023s}{Crüwell et al., 2023}; \protect\hyperlink{ref-hardwicke2021analytic}{Hardwicke et al., 2021}). To meet these challenges, we provide a structured SQL-based schema that organizes data at multiple levels---including publications, studies, participants, and individual trials. This structure allows for fine-grained analysis while maintaining consistency across diverse datasets.

We also provide tools to facilitate access and use of the database. In particular, the R package \texttt{acdcquery} (\protect\hyperlink{ref-R-acdcquery}{Lesche, 2025}) allows users to extract data from TED with flexible filtering criteria, supporting both simple and complex queries. Additionally, we have developed a web-based data entry interface that automatically checks for consistency and flags incorrect variable names or entries (\url{https://slesche.github.io/truth-effect-database/}). This helps maintain high standards for incoming data and ensures that TED remains a reliable resource for the community, while simplifying the submission process and lowering the barrier to entry.

\textbf{Kannst du zu den folgenden Punkten bzgl. interpretation noch deine gedanken ergänzen? Ich habe schonmal ein bisschen die Struktur überlkegt, aber ich glaube das wird besser, wenn du da noch was zu sagst}

To illustrate the analytical potential of TED, we conducted multilevel models estimating random effects at the subject, statement, and publication levels to investigate where variance in the truth effect originates. This approach is only feasible with a dataset of TED's size and granularity.

In both the Likert-scale and dichotomous truth judgment data, we found that the random slopes of repetition on the statement level showed very little variance. This suggests that specific properties of individual statements (e.g., their semantic content or plausibility) have limited influence on the magnitude of the truth effect. However, statement-level variance in random intercepts was relatively large. This is consistent with expectations: some statements are objectively easier to judge as true or false, while others are more ambiguous, leading to baseline differences in overall truth ratings.

We also observed substantial variance in the publication-level random effect of repetition. This is unsurprising, as studies varied considerably in design features such as the number of statements, delay between exposure and judgment, or whether participants were warned about repetition. Nevertheless, the publication-level variance in the effect of repetition was smaller than at the subject level.

Indeed, the largest variance in the truth effect emerged at the subject level, indicating substantial individual differences in susceptibility to the repetition effect. This finding aligns with growing interest in understanding the psychological traits or cognitive mechanisms that moderate the truth effect at the individual level. Some studies \textbf{include studies here}.

However, efforts to predict individual differences in the truth effect have so far produced mixed results \textbf{studies}.

Thus, it remains an open question whether the variability in the truth effect reflects reliable, trait-like differences \textbf{something with issues in other experimental tasks, Rouder \& haaf 2019, Haaf 2025, Hedge 2018, Rouder 2023} \textbf{point to new preprint by Schnuerch},. TED provides a promising platform for revisiting the question of reliable individual differences in the truth effect in future work.

Lastly, we explored whether the magnitude of variance in the repetition effect differed depending on the delay between exposure and judgment---a key procedural variable in truth effect research. Schnuerch et al. (\protect\hyperlink{ref-schnuerch2021truth}{2021}) previously observed reduced individual differences in experiments with longer retention intervals. TED allowed us to extend their analysis to all included studies, estimating variability in the truth effect when the exposure and judgment occur on the same day as opposed to a delay of at least a day while taking variance at the procedure and statement level into account. Something about what we expected

In line with this expectation, we found that the variance in the random effect of repetition was indeed larger when judgments were made on the same day as exposure, compared to when judgments were delayed to a later day. Bayesian model analysis confirmed that this difference is indeed different from 0, indicating true differences in the subject-level variability of the truth effect depending on the delay between exposure and judgment phase.

\textbf{Interpretation, memory effects etc. }

However, these exploratory results should be interpreted cautiously. We did not control for potential confounding factors such as sample composition, procedural variation, or other study-level covariates. \textbf{Something, in time with more data, this will become more representative. But it already points in one direction}

Taken together, these initial analyses demonstrate TED's utility for modeling complex interactions and variance structures that would be difficult to assess using single-study data. The open, extensible nature of TED ensures that these questions can be revisited and refined as the database continues to grow. We see this as a first step toward more nuanced investigations of the truth effect grounded in large-scale, reproducible data.

\hypertarget{future-research}{%
\subsection{Future research}\label{future-research}}

TED offers a unique foundation for advancing research through open, cumulative, and reproducible science. As the database grows in scope and depth, we anticipate several promising directions for future research and methodological development that TED is particularly well-positioned to support.

One immediate application lies in enabling living meta-analyses of truth effect data that can evolve as new studies are contributed. \textbf{maybe add that this is different form meta-analysis, as the data is not true sample of all ITE studies, add that we provide this in the overview website} Closely related, TED can support customized power analyses based on empirical variance from real experimental data. Researchers designing new studies can use TED's trial-level data to simulate expected effects based on previously collected data with similar study characteristics, improving the precision and efficiency of future study planning.

TED's reproducibility potential lies in its ability to support both the replication and reanalysis of existing studies using openly available, trial-level data. Researchers can directly replicate earlier truth effect experiments by matching procedural details drawn from the database, or they can reanalyze existing datasets to test new hypotheses, apply alternative statistical models, or verify published results. This facilitates both direct and conceptual replications and provides a foundation for transparent, data-driven reassessment of prior findings.

Beyond replication, TED also enables the development of formal cognitive models of the truth effect. With detailed trial-level data and metadata on procedural variables, researchers can test and compare models such as drift diffusion models (\protect\hyperlink{ref-ratcliff2008diffusion}{Ratcliff \& McKoon, 2008}) or multinomial processing tree models (\protect\hyperlink{ref-calio2020explicit}{Calio et al., 2020}; \protect\hyperlink{ref-unkelbach2009multinomial}{Unkelbach \& Stahl, 2009}) of truth judgments. The scale and granularity of TED make it possible to fit hierarchical models across subjects, statements, or studies, and to split data into independent training and testing sets---supporting robust model evaluation and cross-validation. This opens the door for developing and benchmarking cognitive theories of truth judgments with strong empirical support.

Importantly, these efforts are supported by TED's intentional focus on long-term sustainability. The database is built with minimal reliance on proprietary infrastructure or paid software. All tools and data are hosted on open platforms, primarily GitHub, and contributions are made through transparent, version-controlled workflows. This lightweight architecture was chosen to promote longevity and ensure that TED remains accessible and modifiable regardless of funding cycles or institutional changes.

To ensure proper attribution and foster community investment, users of TED data are expected to cite all original publications corresponding to the datasets they use. In return, contributions to TED not only extend the utility of existing research but also amplify the visibility of individual studies, creating a mutually reinforcing incentive structure for both data sharing and reuse.

As TED continues to grow, we encourage researchers not only to contribute data but also to build on TED's infrastructure itself. TED itself is an extension of the Attentional Control Data Collection (ACDC) project (\protect\hyperlink{ref-haaf2025attentional}{Haaf et al., 2025}), improving on data submission and ease-of-use. Both projects can serve as a jumping-off point for future database development. All aspects of our project---including the SQL schema, submission interface, R extraction package, and the interactive overview website---are open source and fully forkable. Researchers are invited to reuse or adapt these components to develop similar infrastructures for other domains of psychological or behavioral science. In doing so, TED can help establish a broader culture of open, scalable, and sustainable data practices in experimental research.

Taken together, these features position TED as a platform for cumulative science: one that supports rigorous replication, sophisticated cognitive modeling, empirical synthesis, and infrastructure reusability while lowering the barriers to collaboration and long-term research development.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

In this paper, we introduced the Truth Effect Database (TED), a structured, large-scale resource for cumulative research on truth judgments. TED includes not only standardized, trial-level data but also open tools for user interaction, including a submission interface and an R package for flexible data extraction.

We illustrated the utility of TED through initial multilevel analyses, which highlighted substantial subject-level variance in the truth effect and provide new evidence that individual differences in susceptibility to the truth effect systematically vary due to the length of the retention interval. These finding point to the need for further theoretical and empirical work on individual differences in susceptibility to repetition.

Beyond this first demonstration, TED provides the foundation for a wide range of future research. These include living meta-analyses that evolve as new data are added, simulation-based power analyses informed by real-world variance, rigorous replication and reanalysis of existing studies, and the development of formal cognitive models. As an open and extensible infrastructure, TED also serves as a blueprint for sustainable, community-driven database development in psychological science.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-R-papaja}{}}%
Aust, F., \& Barth, M. (2022). \emph{{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}}. \url{https://github.com/crsh/papaja}

\leavevmode\vadjust pre{\hypertarget{ref-R-lme4}{}}%
Bates, D., Mächler, M., Bolker, B., \& Walker, S. (2015). Fitting linear mixed-effects models using {lme4}. \emph{Journal of Statistical Software}, \emph{67}(1), 1--48. \url{https://doi.org/10.18637/jss.v067.i01}

\leavevmode\vadjust pre{\hypertarget{ref-bauer2022psychological}{}}%
Bauer, P. J. (2022). \emph{Psychological science stepping up a level} (2; Vol. 33, pp. 179--183). SAGE Publications Sage CA: Los Angeles, CA.

\leavevmode\vadjust pre{\hypertarget{ref-R-brms_a}{}}%
Bürkner, P.-C. (2017). {brms}: An {R} package for {Bayesian} multilevel models using {Stan}. \emph{Journal of Statistical Software}, \emph{80}(1), 1--28. \url{https://doi.org/10.18637/jss.v080.i01}

\leavevmode\vadjust pre{\hypertarget{ref-R-brms_b}{}}%
Bürkner, P.-C. (2018). Advanced {Bayesian} multilevel modeling with the {R} package {brms}. \emph{The R Journal}, \emph{10}(1), 395--411. \url{https://doi.org/10.32614/RJ-2018-017}

\leavevmode\vadjust pre{\hypertarget{ref-R-brms_c}{}}%
Bürkner, P.-C. (2021). Bayesian item response modeling in {R} with {brms} and {Stan}. \emph{Journal of Statistical Software}, \emph{100}(5), 1--54. \url{https://doi.org/10.18637/jss.v100.i05}

\leavevmode\vadjust pre{\hypertarget{ref-calio2020explicit}{}}%
Calio, F., Nadarevic, L., \& Musch, J. (2020). How explicit warnings reduce the truth effect: {A} multinomial modeling approach. \emph{Acta Psychologica}, \emph{211}, 103185.

\leavevmode\vadjust pre{\hypertarget{ref-cruwell2023s}{}}%
Crüwell, S., Apthorp, D., Baker, B. J., Colling, L., Elson, M., Geiger, S. J., Lobentanzer, S., Monéger, J., Patterson, A., Schwarzkopf, D. S., et al. (2023). What's in a badge? {A} computational reproducibility investigation of the open data badge policy in one issue of {Psychological Science}. \emph{Psychological Science}, \emph{34}(4), 512--522.

\leavevmode\vadjust pre{\hypertarget{ref-fosterOpenScienceFramework2017}{}}%
Foster, E. D., \& Deardorff, A. (2017). Open {Science Framework} ({OSF}). \emph{Journal of the Medical Library Association}, \emph{105}(2). \url{https://doi.org/10.5195/jmla.2017.88}

\leavevmode\vadjust pre{\hypertarget{ref-gorgolewski2016brain}{}}%
Gorgolewski, K. J., Auer, T., Calhoun, V. D., Craddock, R. C., Das, S., Duff, E. P., Flandin, G., Ghosh, S. S., Glatard, T., Halchenko, Y. O., et al. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. \emph{Scientific Data}, \emph{3}(1), 1--9.

\leavevmode\vadjust pre{\hypertarget{ref-haaf2025attentional}{}}%
Haaf, J. M., Hoffstadt, M., \& Lesche, S. (2025). Attentional control data collection: {A} resource for efficient data reuse. \emph{Behavior Research Methods}, \emph{57}(8), 208.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2021analytic}{}}%
Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., DeMayo, B. E., Long, B., Yoon, E. J., \& Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal {Psychological Science}: {An} observational study. \emph{Royal Society Open Science}, \emph{8}(1), 201494.

\leavevmode\vadjust pre{\hypertarget{ref-hendersonReproducibleSystematicMap2022}{}}%
Henderson, E. L., Westwood, S. J., \& Simons, D. J. (2022). A reproducible systematic map of research on the illusory truth effect. \emph{Psychonomic Bulletin \& Review}, \emph{29}(3), 1065--1088. \url{https://doi.org/10.3758/s13423-021-01995-w}

\leavevmode\vadjust pre{\hypertarget{ref-sqlite2020hipp}{}}%
Hipp, R. D. (2020). \emph{{SQLite}} (Version 3.31.1).

\leavevmode\vadjust pre{\hypertarget{ref-ioannidis2005most}{}}%
Ioannidis, J. P. (2005). Why most published research findings are false. \emph{PLoS Medicine}, \emph{2}(8), e124.

\leavevmode\vadjust pre{\hypertarget{ref-R-acdcquery}{}}%
Lesche, S. (2025). \emph{Acdcquery: Query the attentional control data collection}. \url{https://github.com/SLesche/acdc-query}

\leavevmode\vadjust pre{\hypertarget{ref-munafo2017manifesto}{}}%
Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Percie du Sert, N., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J., \& Ioannidis, J. P. (2017). A manifesto for reproducible science. \emph{Nature Human Behaviour}, \emph{1}(1), 0021.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2015promoting}{}}%
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G., et al. (2015). Promoting an open research culture. \emph{Science}, \emph{348}(6242), 1422--1425.

\leavevmode\vadjust pre{\hypertarget{ref-poldrack2024past}{}}%
Poldrack, R. A., Markiewicz, C. J., Appelhoff, S., Ashar, Y. K., Auer, T., Baillet, S., Bansal, S., Beltrachini, L., Benar, C. G., Bertazzoli, G., et al. (2024). The past, present, and future of the brain imaging data structure ({BIDS}). \emph{Imaging Neuroscience}, \emph{2}, 1--19.

\leavevmode\vadjust pre{\hypertarget{ref-R-base}{}}%
R Core Team. (2022). \emph{R: A language and environment for statistical computing}. R Foundation for Statistical Computing. \url{https://www.R-project.org/}

\leavevmode\vadjust pre{\hypertarget{ref-ratcliff2008diffusion}{}}%
Ratcliff, R., \& McKoon, G. (2008). The diffusion decision model: Theory and data for two-choice decision tasks. \emph{Neural Computation}, \emph{20}(4), 873--922.

\leavevmode\vadjust pre{\hypertarget{ref-schnuerch2021truth}{}}%
Schnuerch, M., Nadarevic, L., \& Rouder, J. N. (2021). The truth revisited: {Bayesian} analysis of individual differences in the truth effect. \emph{Psychonomic Bulletin \& Review}, \emph{28}(3), 750--765.

\leavevmode\vadjust pre{\hypertarget{ref-unkelbach2009multinomial}{}}%
Unkelbach, C., \& Stahl, C. (2009). A multinomial modeling approach to dissociate different components of the truth effect. \emph{Consciousness and Cognition}, \emph{18}(1), 22--38.

\leavevmode\vadjust pre{\hypertarget{ref-R-tidyverse}{}}%
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., \ldots{} Yutani, H. (2019). Welcome to the {tidyverse}. \emph{Journal of Open Source Software}, \emph{4}(43), 1686. \url{https://doi.org/10.21105/joss.01686}

\leavevmode\vadjust pre{\hypertarget{ref-wilkinson2016fair}{}}%
Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L. B., Bourne, P. E., et al. (2016). The {FAIR Guiding Principles} for scientific data management and stewardship. \emph{Scientific Data}, \emph{3}(1), 1--9.

\leavevmode\vadjust pre{\hypertarget{ref-R-knitr}{}}%
Xie, Y. (2015). \emph{Dynamic documents with {R} and knitr} (2nd ed.). Chapman; Hall/CRC. \url{https://yihui.org/knitr/}

\end{CSLReferences}

\newpage

\hypertarget{appendix-appendix}{%
\appendix}


\hypertarget{talking-about-appendices}{%
\section{Talking about appendices}\label{talking-about-appendices}}


\end{document}
