# Results
```{r method-source-data-prep, child = "markdown/data_prep.rmd"}
```
```{r method-source-data-anal, child = "markdown/data_analysis.rmd"}
```

All analysis were conducted using `r r_citations$r`. `r r_citations$pkgs`.

In the current version of the manuscript, we included `r nrow(study_overview) %>% apa_num()` studies from `r nrow(publications_overview) %>% apa_num()` publications, spanning `r length(unique(full_data$subject)) %>% apa_num()` participants contributing `r nrow(full_data) %>% apa_num()` trials. A complete list of the included publications can be found in Table **table with refs, studies, trials here**.

```{r}
age_data <- study_overview %>% 
  mutate(participant_age = ifelse(participant_age == 99, NA, participant_age)) %>% 
  filter(!is.na(participant_age), !is.na(n_participants))
```

Sample composition ranged from `r min(study_overview$n_participants, na.rm = TRUE)` to `r max(study_overview$n_participants, na.rm = TRUE)` participants. On average, studies included `r mean(study_overview$n_participants, na.rm = TRUE)` participants ($\mu_{age} =$ `r weighted.mean(age_data$participant_age, age_data$n_participants, na.rm = TRUE)`,$\sigma_{age} =$ `r DescTools::SD(age_data$participant_age, weights = age_data$n_participants, na.rm = TRUE)`). An overview of the rating scale usage for truth judgments and the use of a filler task over all included studies can be found in Figure \@ref(fig:study-overview-plot).

(ref:study-overview-plot) Overview of Study-related variables in TED
```{r study-overview-plot,  fig.cap = paste("(ref:study-overview-plot)"), out.width="90%"}
knitr::include_graphics("images/study_overview_plot.png")
```

On average, studies employed `r mean(procedure_data$n_statements)` ($SD =$ `r sd(procedure_data$n_statements)`) statements per 
participant in the judgment session and in `r print_freq_percent(sum(procedure_data$percent_repeated == 50) / nrow(procedure_data))` of procedure settings exactly 50% of statements were repeated. Of `r nrow(procedure_data)` judgment phases, `r print_freq_percent(sum(procedure_data$repetition_time < 180) / nrow(procedure_data))` were conducted on the same day as the exposure phase. The average delay between exposure and judgment phase if both were conducted on the same day was `r procedure_data %>% filter(repetition_time < 180) %>% pull(repetition_time) %>% mean()` minutes. The average delay between exposure and judgment phase, given the judgment phase was conducted at least one day after the exposure phase, was `r round((procedure_data %>% filter(repetition_time > 180) %>% pull(repetition_time) %>% mean())/(60*24), 2)` days. An overview of additional variables pertaining to the procedure of the included studies can be found in Figure \@ref(fig:procedure-overview-plot).

<!-- An overview of the delay between exposure and judgment phase is provided in Figure \@ref(fig:delay-overview-plot).  -->

<!-- (ref:delay-overview-plot) Overview of delay between exposure and test phase -->
<!-- ```{r delay-overview-plot, fig.cap = paste("(ref:delay-overview-plot)"), out.width="100%"} -->
<!-- knitr::include_graphics("images/delay_overview_plot.png") -->
<!-- ``` -->

(ref:procedure-overview-plot) Overview of Procedure-related variables in TED
```{r procedure-overview-plot,  fig.cap = paste("(ref:procedure-overview-plot)"), out.width="100%"}
knitr::include_graphics("images/procedure_overview_plot.png")
```

Detailed information on the statements presented is available for `r study_overview %>% filter(statementset_id != 1) %>% nrow()` out of `r study_overview %>% nrow()` studies. Data on the accuracy of a statement is available for `r (length(analysis_data$statement_accuracy[which(!is.na(analysis_data$statement_accuracy))]) / nrow(analysis_data)) %>% print_freq_percent()` of trials, the detailed statement text is available for `r (length(analysis_data$statement_text[which(!is.na(analysis_data$statement_text))]) / nrow(analysis_data)) %>% print_freq_percent()` of trials, and response times are available for `r (length(analysis_data$rt[which(!is.na(analysis_data$rt))]) / nrow(analysis_data)) %>% print_freq_percent()` of trials.

## Multi-level Modelling

To illustrate the benefits of our large collection of trial-level data, we applied multilevel models predicting truth judgments with repetition as a fixed effect and random intercepts and slopes at the subject, statement, and procedure levels.

We analyzed the dichotomous and Likert-type response formats separately due to differences in their scale characteristics. Dichotomous responses (e.g., true/false) require logistic models, whereas Likert-type responses (e.g., 1â€“5 ratings) allow for linear models. All responses were maximum-normalized to the range 0-1 with one representing the maximum possible response indicating a "true" judgment.

We fit five models per dataset. A baseline model with only random intercepts. A full model including random intercepts and random slopes for repetition across all grouping factors. And three reduced models, each excluding the random slope of repetition for one grouping factor (subject, statement, or procedure).

### Dichotomous Truth Judgments
The analysis was based on `r nrow(dichotomous_data) %>% apa_num()` trials nested within `r length(unique(dichotomous_data$subject)) %>% apa_num()` subjects, `r length(unique(dichotomous_data$statement_id)) %>% apa_num()` statements, and `r length(unique(dichotomous_data$procedure_id)) %>% apa_num()` procedures.

(ref:model-comp-dichotomous) Overview of Information Criteria. Models...

```{r model-comp-dichotomous,  tab.cap = paste("(ref:model-comp-dichotomous)"), out.width="100%"}
papaja::apa_table(model_comp_dichotomous)
```

All models that included a random effect of repetition at any level outperformed the baseline model with only random intercepts. Detailed model comparisons, based on information criteria, are presented in Table \@ref(tab:model-comp-dichotomous).

The model incorporating random effects at all hierarchical levels provided the best overall fit **here, fit indices!**. Compared to the model excluding random effects of statements, it demonstrated a slightly lower AIC and a marginally higher BIC. This is attributable to the minimal variance observed in the repetition effect at the statement level. Notably, the variance of the random effect of repetition was highest at the subject level, followed by the procedure level.

### Scale Truth Judgments
The analysis was based on `r nrow(scale_data) %>% apa_num()` trials nested within `r length(unique(scale_data$subject)) %>% apa_num()` subjects, `r length(unique(scale_data$statement_id)) %>% apa_num()` statements, and `r length(unique(scale_data$procedure_id)) %>% apa_num()` procedures.

We employed the "bobyqa" optimizer, as some models initially failed to converge, and estimated the models using full maximum likelihood. An overview of the information criteria for all evaluated models is presented in Table \@ref(tab:model-comp-scale).

(ref:model-comp-scale) Overview of Information Criteria. Models...
```{r model-comp-scale,  tab.cap = paste("(ref:model-comp-scale)"), out.width="90%"}
papaja::apa_table(model_comp_scale)
```

The model including random effects at all hierarchical levels provided the best overall fit **here, fit indices!!**, exhibiting improved AIC and BIC values compared to the model that excluded random effects of statements. As in the dichotomous analysis, the variance of the repetition effect was near zero at the statement level, while being most pronounced at the subject level.

To further explore the influence of temporal delay between the exposure and judgment phases on interindividual variability in the repetition effect, we stratified the data. Specifically, we re-estimated the multilevel model (with random effects at all levels) separately for judgments made on the same day as the exposure phase and for those made on a subsequent day.

The results of this model can be found in table **table!**

Fit indices and effect sizes of these models. Variance measures here, too.

Model Sameday
```{r}
model_re_likert_sameday
```

Model Later
```{r}
model_re_likert_later
```

The variance of the random effect of repetition was higher in the data containing truth judgments from the same day as the exposure phase (number 1) vs. (number 2).