# Introduction
The open science movement has emerged in response to growing concerns about the replicability, transparency, and efficiency of scientific research [@ioannidis2005most; @nosek2015promoting; @munafo2017manifesto]. At its core, open science promotes practices that make research processes and outputs accessible, verifiable, and reusable. Practices such as preregistration, data sharing, and open materials aim to increase the trustworthiness of findings and to foster a more cumulative, collaborative scientific enterprise. These practices, coupled with platforms like the Open Science Framework - OSF [@fosterOpenScienceFramework2017], have played a pivotal role in advancing openness through tools that support study registration, version control, and public sharing of data and materials. Transparency, once rare, is increasingly becoming the norm [@bauer2022psychological].

However, while transparency has improved markedly, efficiency and reusability have lagged behind. Research materials and datasets are often siloed within individual project pages, tailored to host single studies. As a result, even when data are technically open, they are rarely standardized in ways that facilitate reuse, integration, or cumulative analysis. Current open data practices focus primarily on the first two aspects of the FAIR (Findable, Accessible, Interoperable, and Reusable) principles of data sharing [@wilkinson2016fair], while neglecting the latter. For example, the OSF is well-suited to documenting and finding individual projects, but its structure does not support the aggregation or harmonization of data across multiple related studies. In addition to structural limitations, researchers' practices like inconsistent file formats, project-specific codebooks, and divergent analytic pipelines continue to hinder interoperability.

This inconsistency not only limits replicability, but also reusability. Should a researcher be interested in estimating an effect size, developing a new computational model or simply re-analyzing existing data instead of collecting an entirely new sample, they have to comb through OSF, pray for existing codebooks and hope for decipherable raw data. This is both tedious and often unsuccessful [@cruwell2023s; @hardwicke2021analytic]. However, reusability greatly improves scientific efficiency. It saves resources otherwise wasted by planning and collecting an entirely new set of data and helps efficiently allocate public resources as well as participants' and researchers' time.

We argue that to make research outputs genuinely reusable, not just transparent, structure is essential. Reusability requires standardized organization: consistent variable naming, common data schemas, uniform documentation practices, and clear, accessible codebooks. In neuroscience, this principle has been successfully realized through the Brain Imaging Data Structure - BIDS [@gorgolewski2016brain], which provides a framework for organizing and annotating neuroimaging data. BIDS has enabled not just more efficient sharing, but also reproducible pipelines, automated analyses, and collaborative efforts at scale [@poldrack2024past].

Structured approaches like BIDS are now gaining traction in cognitive psychology. The Attentional Control Data Collection (ACDC) introduced a trial-level database for data from attentional control experiments, providing standardized variable names, metadata, and analysis-ready formats that streamline cross-study comparisons [@haaf2025attentional]. Similarly, the FEARBASE project is building a large-scale, open-access repository for fear conditioning studies, adopting a shared structure to ensure comparability and long-term usability (Lohnsdorf & Ehlers, 2025). Crucially, there is increasing institutional support for the development of infrastructure enabling data reuse. For example, the German research foundation *DFG* issued an open funding call for projects developing infrastructure for scientific data management (DFG; *Förderprogramm Informationsinfrastrukturen für Forschungsdaten*). These projects spearhead a changing culture of data-reusability but so far represent the exception, rather than the norm.

We believe that there is great potential in the development of structured databases integrating trial level data. Large and structured collections of raw data have several key applications, extending the general principle of data-reusability. They enable living meta-analyses that can update automatically as new data are added; allow researchers to find relevant raw data based on task characteristics or participant variables; facilitate the straightforward replication of published findings; and enable power analyses using pooled datasets from comparable studies.

Beyond these direct applications, structured datasets also support the development of new methods and models. Trial-level repositories allow for benchmarking existing analytic tools, training machine learning models, and exploring novel statistical approaches.

To illustrate the potential of such structured and reusable databases, we focus on a particularly relevant and empirically rich phenomenon in cognitive and social psychology: the illusory truth effect (ITE). The ITE has been robustly demonstrated across numerous studies, yet substantial heterogeneity in effect sizes, experimental designs, and individual-level moderators complicates efforts to draw generalizable conclusions. These complexities make it an ideal case for a centralized, harmonized repository of trial-level data. In the following section, we provide a brief overview of the ITE and explain how its empirical challenges underscore the need for exactly the kind of infrastructure we propose.

### The illusory truth effect
In today’s data-saturated and fast-paced information environment, individuals are confronted with an overwhelming amount of content, often of uncertain or questionable credibility. The increasing prevalence of misinformation, disinformation, and strategically disseminated fake news poses a serious challenge to our societies. As incoming information continuously forms human attitudes and beliefs, mechanisms that shape the subjective truth of information become central to understanding both individual and societal processes.

One particularly robust and well-documented phenomenon in this context is the illusory truth effect (ITE) – the tendency to perceive repeated information as more credible, regardless of its actual accuracy. Initially experimentally demonstrated by Hasher et al. (1977), this effect has been consistently observed in over 80 replications.  Previous research indicates that the illusory truth effect is primarily driven by increased processing fluency as a result of repetition. That is, when evaluating the truthfulness of a statement, individuals often draw on their subjective sense of ease during processing as a heuristic cue (e.g., Dechêne et al., 2010). Crucially, the relative fluency appears to be decisive: Dechêne et al. (2009) showed that the effect only reliably occurs when fluent and disfluent stimuli are presented together. In experimental paradigms investigating the ITE, fluency is typically manipulated by contrasting new statements with repeated ones which have been shown during a prior exposure phase. 
Although ITE has been observed across various domains and populations, prior research indicates that the illusory truth effect significantly varies among individuals (e.g., Nadarevic, 2010; Schnuerch et al., 2021). 

Research examining dispositional factors that may account for individual differences in susceptibility to the illusory truth effect has yielded mixed and often inconclusive findings. For example, Arkes et al. (1991) and Boehm (1994) investigated need for cognition (NFC) as a potential trait moderator of the effect but found no supporting evidence. Similarly, Newman et al. (2020) explored whether NFC moderates the illusory truth effect and found that this relationship was contingent upon the nature of the experimental instructions. Specifically, the moderating influence of NFC emerged only when participants were not informed that the exposure phase contained both true and false statements; when such information was provided, the effect was no longer evident. In addition to NFC, other dispositional factors have been investigated with similarly inconsistent results. For instance, Kim (2002) examined whether skepticism moderates the effect and reported ambiguous findings, while DiFonzo et al. (2016) found marginal evidence for such moderation. Sundar et al. (2015), focusing on repeated health-related messages, observed that individuals with a high need for affect (i.e., the tendency to approach or avoid affect-inducing situations) were more susceptible to the ITE. More recently, De keersmaecker et al. (2020) tested three further individual difference variables – need for cognitive closure, thinking style preference (intuitive vs. deliberative), and cognitive ability – but found no evidence that any of these measures moderated the truth effect.  In two experiments, Stump et al. (2022) found evidence suggesting moderating effects for NCC on the illusory truth effect after a relatively short but not after a longer retention interval (10 minutes vs. 1 week). Crucially, De keersmaecker et al. (2020) assessed NCC exclusively in one study, which employed a single, relatively long retention interval of five to seven days. The pattern of findings suggests that the temporal distance between exposure and judgment phase may be a key factor when assessing the role of individual differences in the truth effect. Further support for this interpretation comes from recent work by Schnuerch et al. (2021), who conducted a comprehensive Bayesian reanalysis of multiple datasets examining individual differences in the illusory truth effect. Their results revealed robust evidence for such differences in five datasets, whereas at least two datasets provided evidence against them. Crucially, in the latter two studies, the retention interval spanned one week, while in the other datasets the judgment phase occurred within the same experimental session as the exposure phase. 

Taken together, these findings offer preliminary insights into how individual differences may shape susceptibility to the truth effect. However, they also highlight the complexity of the phenomenon and suggest that an integrative approach – considering both dispositional and contextual variables – is essential for a more comprehensive understanding of the mechanisms underlying the ITE. 

### The present study
In the spirit of providing FAIR data to aid research on ITE, we introduce a centralized, trial-level database, drawing on resources and lessons learned from developing ACDC [@haaf2025attentional]. The truth-effect database (TED) is designed not only with structured organization in mind, but also with an emphasis on ease of use. Particular attention has been given to lowering the barrier for data submission through an intuitive entry-mask, as well as to enabling simple data extraction via an R package. Our aim is to create a living, extensible resource that supports both contributors and users: researchers can add new studies with minimal friction, and others can search, filter, and analyze trial-level data without having to clean or realign disparate datasets. By combining structure with usability, this resource is intended to foster cumulative research on the illusory truth effect and to serve as a model for reusable psychological data infrastructures more broadly.


