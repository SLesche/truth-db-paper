# Discussion
In this paper, we introduce the Truth Effect Database (TED), a large-scale, open-access repository of trial-level data on truth judgments. TED is designed to support cumulative, reproducible research on the truth effect by standardizing and centralizing data from a wide range of studies. It currently aggregates data from `r nrow(publications_overview) %>% apa_num()` publications, spanning `r length(unique(full_data$subject)) %>% apa_num()` participants and `r nrow(full_data) %>% apa_num()` trials, accounting for a wide range of dispositional and contextual variables – including experimental instructions, task during initial exposure, number of repetitions, and length of the retention interval.  With its growing size, TED provides a critical resource for investigating how repetition shapes belief formation – a process of increasing relevance in the digital age, where repeated exposure to false or misleading information is pervasive.

Our goal in developing TED was to address key challenges in open research data: the fragmentation of data across studies, the lack of standard formats for sharing experimental data, and the resulting lack of interoperability and reusability [@cruwell2023s; @hardwicke2021analytic]. To meet these challenges, we provide a structured SQL-based schema that organizes data at multiple levels—including publications, studies, participants, and individual trials. This structure allows for fine-grained analysis while maintaining consistency across diverse datasets.

We also provide tools to facilitate access and use of the database. In particular, the R package `acdcquery` [@R-acdcquery] allows users to extract data from TED with flexible filtering criteria, supporting both simple and complex queries. Additionally, we have developed a web-based data entry interface that automatically checks for consistency and flags incorrect variable names or entries (https://slesche.github.io/truth-effect-database/). This helps maintain high standards for incoming data and ensures that TED remains a reliable resource for the community, while simplifying the submission process and lowering the barrier to entry.

**Kannst du zu den folgenden Punkten bzgl. interpretation noch deine gedanken ergänzen? Ich habe schonmal ein bisschen die Struktur überlkegt, aber ich glaube das wird besser, wenn du da noch was zu sagst** 

To illustrate the analytical potential of TED, we conducted multilevel models estimating random effects at the subject, statement, and publication levels to investigate where variance in the truth effect originates. This approach is only feasible with a dataset of TED’s size and granularity.

In both the Likert-scale and dichotomous truth judgment data, we found that the random slopes of repetition on the statement level showed very little variance. This suggests that specific properties of individual statements (e.g., their semantic content or plausibility) have limited influence on the magnitude of the truth effect. However, statement-level variance in random intercepts was relatively large. This is consistent with expectations: some statements are objectively easier to judge as true or false, while others are more ambiguous, leading to baseline differences in overall truth ratings.

We also observed substantial variance in the publication-level random effect of repetition. This is unsurprising, as studies varied considerably in design features such as the number of statements, delay between exposure and judgment, or whether participants were warned about repetition. Nevertheless, the publication-level variance in the effect of repetition was smaller than at the subject level.

Indeed, the largest variance in the truth effect emerged at the subject level, indicating substantial individual differences in susceptibility to the repetition effect. This finding aligns with growing interest in understanding the psychological traits or cognitive mechanisms that moderate the truth effect at the individual level. Some studies **include studies here**. 

However, efforts to predict individual differences in the truth effect have so far produced mixed results **studies**. 

Thus, it remains an open question whether the variability in the truth effect reflects reliable, trait-like differences **something with issues in other experimental tasks, Rouder & haaf 2019, Haaf 2025, Hedge 2018, Rouder 2023** **point to new preprint by Schnuerch**,. TED provides a promising platform for revisiting the question of reliable individual differences in the truth effect in future work.

Lastly, we explored whether the magnitude of variance in the repetition effect differed depending on the delay between exposure and judgment—a key procedural variable in truth effect research. @schnuerch2021truth previously observed reduced individual differences in experiments with longer retention intervals. TED allowed us to extend their analysis to all included studies, estimating variability in the truth effect when the exposure and judgment occur on the same day as opposed to a delay of at least a day while taking variance at the procedure and statement level into account. Something about what we expected

In line with this expectation, we found that the variance in the random effect of repetition was indeed larger when judgments were made on the same day as exposure, compared to when judgments were delayed to a later day. Bayesian model analysis confirmed that this difference is indeed different from 0, indicating true differences in the subject-level variability of the truth effect depending on the delay between exposure and judgment phase.

**Interpretation, memory effects etc. **

However, these exploratory results should be interpreted cautiously. We did not control for potential confounding factors such as sample composition, procedural variation, or other study-level covariates. **Something, in time with more data, this will become more representative. But it already points in one direction**

Taken together, these initial analyses demonstrate TED’s utility for modeling complex interactions and variance structures that would be difficult to assess using single-study data. The open, extensible nature of TED ensures that these questions can be revisited and refined as the database continues to grow. We see this as a first step toward more nuanced investigations of the truth effect grounded in large-scale, reproducible data.

## Future research

TED offers a unique foundation for advancing research through open, cumulative, and reproducible science. As the database grows in scope and depth, we anticipate several promising directions for future research and methodological development that TED is particularly well-positioned to support.

One immediate application lies in enabling living meta-analyses of truth effect data that can evolve as new studies are contributed. **maybe add that this is different form meta-analysis, as the data is not true sample of all ITE studies, add that we provide this in the overview website** Closely related, TED can support customized power analyses based on empirical variance from real experimental data. Researchers designing new studies can use TED's trial-level data to simulate expected effects based on previously collected data with similar study characteristics, improving the precision and efficiency of future study planning.

TED’s reproducibility potential lies in its ability to support both the replication and reanalysis of existing studies using openly available, trial-level data. Researchers can directly replicate earlier truth effect experiments by matching procedural details drawn from the database, or they can reanalyze existing datasets to test new hypotheses, apply alternative statistical models, or verify published results. This facilitates both direct and conceptual replications and provides a foundation for transparent, data-driven reassessment of prior findings.

Beyond replication, TED also enables the development of formal cognitive models of the truth effect. With detailed trial-level data and metadata on procedural variables, researchers can test and compare models such as drift diffusion models [@ratcliff2008diffusion] or multinomial processing tree models [@unkelbach2009multinomial; @calio2020explicit] of truth judgments. The scale and granularity of TED make it possible to fit hierarchical models across subjects, statements, or studies, and to split data into independent training and testing sets—supporting robust model evaluation and cross-validation. This opens the door for developing and benchmarking cognitive theories of truth judgments with strong empirical support.

Importantly, these efforts are supported by TED’s intentional focus on long-term sustainability. The database is built with minimal reliance on proprietary infrastructure or paid software. All tools and data are hosted on open platforms, primarily GitHub, and contributions are made through transparent, version-controlled workflows. This lightweight architecture was chosen to promote longevity and ensure that TED remains accessible and modifiable regardless of funding cycles or institutional changes.

To ensure proper attribution and foster community investment, users of TED data are expected to cite all original publications corresponding to the datasets they use. In return, contributions to TED not only extend the utility of existing research but also amplify the visibility of individual studies, creating a mutually reinforcing incentive structure for both data sharing and reuse.

As TED continues to grow, we encourage researchers not only to contribute data but also to build on TED’s infrastructure itself. TED itself is an extension of the Attentional Control Data Collection (ACDC) project [@haaf2025attentional], improving on data submission and ease-of-use. Both projects can serve as a jumping-off point for future database development. All aspects of our project—including the SQL schema, submission interface, R extraction package, and the interactive overview website—are open source and fully forkable. Researchers are invited to reuse or adapt these components to develop similar infrastructures for other domains of psychological or behavioral science. In doing so, TED can help establish a broader culture of open, scalable, and sustainable data practices in experimental research.

Taken together, these features position TED as a platform for cumulative science: one that supports rigorous replication, sophisticated cognitive modeling, empirical synthesis, and infrastructure reusability while lowering the barriers to collaboration and long-term research development.

## Conclusion
In this paper, we introduced the Truth Effect Database (TED), a structured, large-scale resource for cumulative research on truth judgments. TED includes not only standardized, trial-level data but also open tools for user interaction, including a submission interface and an R package for flexible data extraction.

We illustrated the utility of TED through initial multilevel analyses, which highlighted substantial subject-level variance in the truth effect and provide new evidence that individual differences in susceptibility to the truth effect systematically vary due to the length of the retention interval. These finding point to the need for further theoretical and empirical work on individual differences in susceptibility to repetition.

Beyond this first demonstration, TED provides the foundation for a wide range of future research. These include living meta-analyses that evolve as new data are added, simulation-based power analyses informed by real-world variance, rigorous replication and reanalysis of existing studies, and the development of formal cognitive models. As an open and extensible infrastructure, TED also serves as a blueprint for sustainable, community-driven database development in psychological science.
