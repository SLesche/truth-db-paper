# Discussion
In this paper, we introduce the Truth-Effect Database (TED), a large-scale, open-access repository of trial-level data on truth judgments. TED is designed to support cumulative, reproducible research on the truth effect by standardizing and centralizing data from a wide range of studies. With contributions from over 10,000 participants, our database offers a comprehensive resource for researchers interested in understanding how repetition and related factors influence truth judgments.

Our goal in developing TED was to address key challenges in open research data: the fragmentation of data across studies, the lack of standard formats for sharing experimental data, and the resulting lack of interoperability and reusability **citations here**. To meet these challenges, we provide a structured SQL-based schema that organizes data at multiple levels—including publications, studies, participants, and individual trials. This structure allows for fine-grained analysis while maintaining consistency across diverse datasets.

We also provide tools to facilitate access and use of the database. In particular, the R package `acdcquery` **cite!!** allows users to extract data with flexible filtering criteria, supporting both simple and complex queries. Additionally, we have developed a web-based data entry interface that automatically checks for consistency and flags incorrect variable names or entries (https://slesche.github.io/truth-effect-database/). This helps maintain high standards for incoming data and ensures that TED remains a reliable resource for the community, while simplifying the submission process and lowering the barrier to entry.

To illustrate the analytical potential of TED, we conducted multilevel models estimating random effects at the subject, statement, and publication levels to investigate where variance in the truth effect originates. This approach is only feasible with a dataset of TED’s size and granularity.

In both the Likert-scale and dichotomous truth judgment data, we found that the random slopes of repetition on the statement level showed very little variance. This suggests that specific properties of individual statements (e.g., their semantic content or plausibility) have limited influence on the magnitude of the truth effect. However, statement-level variance in random intercepts was relatively large. This is consistent with expectations: some statements are objectively easier to judge as true or false, while others are more ambiguous, leading to baseline differences in overall truth ratings.

We also observed substantial variance in the publication-level random effect of repetition. This is unsurprising, as studies varied considerably in design features such as the number of statements, delay between exposure and judgment, or whether participants were warned about repetition. Nevertheless, the publication-level variance in the effect of repetition was smaller than at the subject level.

Indeed, the largest variance in the truth effect emerged at the subject level, indicating substantial individual differences in susceptibility to the repetition effect. This finding aligns with growing interest in understanding the psychological traits or cognitive mechanisms that moderate the truth effect at the individual level. Some studies **include studies here**. 

However, efforts to predict individual differences in the truth effect have so far produced mixed results **studies**. 

Thus, it remains an open question whether the variability in the truth effect reflects reliable, trait-like differences. TED provides a promising platform for revisiting the question of reliable individual differences in the truth effect in future work.

Lastly, we explored whether the magnitude of variance in the repetition effect differed depending on the delay between exposure and judgment—a key procedural variable in truth effect research. Based on prior studies **include study**, we expected variability in the truth effect to be greater when the exposure and judgment occur on the same day, as repetition-based fluency is likely more salient in short-delay conditions.

**also the overall size of truth effect here?**
In line with this expectation, we found that the variance in the random effect of repetition was indeed larger when judgments were made on the same day as exposure, compared to when judgments were delayed to a later day. 
**Interpretation **

However, these exploratory results should be interpreted cautiously. We did not test the delay-related differences for significance, and the analysis did not control for potential confounding factors such as sample composition, procedural variation, or other study-level covariates. **Something, in time with more data, this will become more representative. But it already points in one direction**

Taken together, these initial analyses demonstrate TED’s utility for modeling complex interactions and variance structures that would be difficult to assess using single-study data. The open, extensible nature of TED ensures that these questions can be revisited and refined as the database continues to grow. We see this as a first step toward more nuanced investigations of the truth effect grounded in large-scale, reproducible data.

## Future research

TED offers a unique foundation for advancing research through open, cumulative, and reproducible science. As the database grows in scope and depth, we anticipate several promising directions for future research and methodological development that TED is particularly well-positioned to support.

One immediate application lies in enabling living meta-analyses of truth effect data that can evolve as new studies are contributed. Closely related, TED can support customized power analyses based on empirical variance from real experimental data. Researchers designing new studies can use TED's trial-level data to simulate expected effects based on previously collected data with similar study characteristics, improving the precision and efficiency of future study planning.

TED’s reproducibility potential lies in its ability to support both the replication and reanalysis of existing studies using openly available, trial-level data. Researchers can directly replicate earlier truth effect experiments by matching procedural details drawn from the database, or they can reanalyze existing datasets to test new hypotheses, apply alternative statistical models, or verify published results. This facilitates both direct and conceptual replications and provides a foundation for transparent, data-driven reassessment of prior findings.

Beyond replication, TED also enables the development of formal cognitive models of the truth effect. With detailed trial-level data and metadata on procedural variables, researchers can test and compare models such as drift diffusion models, multinomial processing tree models, or signal detection accounts of truth judgments. The scale and granularity of TED make it possible to fit hierarchical models across subjects, statements, or studies, and to split data into independent training and testing sets—supporting robust model evaluation and cross-validation. This opens the door for developing and benchmarking cognitive theories of truth judgments with strong empirical support.

Importantly, these efforts are supported by TED’s intentional focus on long-term sustainability. The database is built with minimal reliance on proprietary infrastructure or paid software. All tools and data are hosted on open platforms, primarily GitHub, and contributions are made through transparent, version-controlled workflows. This lightweight architecture was chosen to promote longevity and ensure that TED remains accessible and modifiable regardless of funding cycles or institutional changes.

To ensure proper attribution and foster community investment, users of TED data are expected to cite all original publications corresponding to the datasets they use. In return, contributions to TED not only extend the utility of existing research but also amplify the visibility of individual studies, creating a mutually reinforcing incentive structure for both data sharing and reuse.

As TED continues to grow, we encourage researchers not only to contribute data but also to build on TED’s infrastructure itself. TED itself is an extension of the Attentional Control Data Collection (ACDC) **cite!!** project, improving on data submission and ease-of-use and can as a blueprint and jumping-off point for future database development. All aspects of the project—including the SQL schema, submission interface, R extraction package, and the interactive overview website—are open source and fully forkable. Researchers are invited to reuse or adapt these components to develop similar infrastructures for other domains of psychological or behavioral science. In doing so, TED can help establish a broader culture of open, scalable, and sustainable data practices in experimental research.

Taken together, these features position TED as a platform for cumulative science: one that supports rigorous replication, sophisticated cognitive modeling, empirical synthesis, and infrastructure reusability while lowering the barriers to collaboration and long-term research development.

## Conclusion
In this paper, we introduced the Truth-Effect Database (TED), a structured, large-scale resource for cumulative research on truth judgments. TED includes not only standardized, trial-level data but also open tools for user interaction, including a submission interface, a data exploration website, and an R package for flexible data extraction.

We illustrated the utility of TED through initial multilevel analyses, which highlighted substantial subject-level variance in the truth effect. This finding points to the need for further theoretical and empirical work on individual differences in susceptibility to repetition.

Beyond this first demonstration, TED provides the foundation for a wide range of future research. These include living meta-analyses that evolve as new data are added, simulation-based power analyses informed by real-world variance, rigorous replication and reanalysis of existing studies, and the development of formal cognitive models. As an open and extensible infrastructure, TED also serves as a blueprint for sustainable, community-driven database development in psychological science.
